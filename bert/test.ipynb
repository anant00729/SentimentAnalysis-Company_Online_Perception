{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# BERT Testing Notebook\n",
    "This notebook contains the code to test the bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aidan/Git-Repositories/nlpclass-1231-g-the_3rd_times_the_charm/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-08 12:40:34.343343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 12:40:34.814719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from my_dataset import My_Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "import time\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the tokenzier to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Defining function to compute the tokenization\n",
    "def tokenize_function(data):\n",
    "    # value = tokenizer(data[\"sequence\"], padding=\"max_length\", truncation=True)\n",
    "    # return value['input_ids'], value['token_type_ids'], value['attention_mask']\n",
    "    return tokenizer(data[\"sequence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def prep_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Defining function that preps the data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying the tokenizer on the inputs\n",
    "    tokenized_values = df.apply(tokenize_function, axis=1)\n",
    "    \n",
    "    # Splitting the results into a dataframe\n",
    "    tokenized_values = tokenized_values.apply(pd.Series)\n",
    "    \n",
    "    # Merging the tokenized values together\n",
    "    df = pd.concat([df,tokenized_values], axis=1)\n",
    "    \n",
    "    # Renaming columns\n",
    "    df = df.rename(columns = {\"label\": \"labels\"})\n",
    "    \n",
    "    # Dropping columns\n",
    "    df = df.drop(['sequence'],axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Returning the dataset\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"checkpoints/bert-base-uncased_4000_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing (All Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the in the splits for each dataset\n",
    "train = pd.read_csv(f\"data/train_{NUM_SAMPLES}.csv\")\n",
    "test = pd.read_csv(f\"data/test_{NUM_SAMPLES}.csv\")\n",
    "val = pd.read_csv(f\"data/val_{NUM_SAMPLES}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepping the data\n",
    "train = prep_data(train)\n",
    "test = prep_data(test)\n",
    "val = prep_data(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the data as pytorch datasets\n",
    "train_dataset = My_Dataset(train)\n",
    "val_dataset = My_Dataset(val)\n",
    "test_dataset = My_Dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining list to hold the results\n",
    "results_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_all.append({\"Data Split\": \"Validation\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Data Split': 'Validation',\n",
       "  'F1': 0.8179487179487179,\n",
       "  'Accuracy': 0.8225,\n",
       "  'Precision': 0.7935323383084577,\n",
       "  'Recall': 0.843915343915344}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_all.append({\"Data Split\": \"Train\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_all.append({\"Data Split\": \"Test\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting to dataframe\n",
    "results_all = pd.DataFrame(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to disk\n",
    "results_all.to_csv(\"results/bert_results_all_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME:  15.02766466140747\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "print(\"TIME: \", t1 - t0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Formulating DF\n",
    "rows = [\n",
    "    {\"Model\": \"BERT\", \"Data Split\": \"Test\", \"Number of Sequences\": 1600, \"Time (seconds)\": 14.875643253326416, \"Sequences per Second\": 107.55837396424629},\n",
    "    {\"Model\": \"LSTM\", \"Data Split\": \"Test\", \"Number of Sequences\": 1600, \"Time (seconds)\": 0.5176246166229248, \"Sequences per Second\": 3091.0431007680527}\n",
    "]\n",
    "\n",
    "time_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rounding values\n",
    "time_df[\"Sequences per Second\"] = round(time_df[\"Sequences per Second\"], 2)\n",
    "time_df[\"Sequences per Second\"] = time_df[\"Sequences per Second\"].astype(str)\n",
    "\n",
    "time_df[\"Time (seconds)\"] = round(time_df[\"Time (seconds)\"], 2)\n",
    "time_df[\"Time (seconds)\"] = time_df[\"Time (seconds)\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the index\n",
    "time_df = time_df.set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/google-chrome\n"
     ]
    }
   ],
   "source": [
    "# Setting styles and writing to disk\n",
    "time_df = time_df.style.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "time_df.set_properties(**{'text-align': 'center'})\n",
    "dfi.export(time_df, 'results/test_timing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing (News Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the in the splits for each dataset\n",
    "test = pd.read_csv(f\"data/test_{NUM_SAMPLES}_news.csv\")\n",
    "val = pd.read_csv(f\"data/val_{NUM_SAMPLES}_news.csv\")\n",
    "train = pd.read_csv(f\"data/train_{NUM_SAMPLES}_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepping the data\n",
    "train = prep_data(train)\n",
    "test = prep_data(test)\n",
    "val = prep_data(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the data as pytorch datasets\n",
    "val_dataset = My_Dataset(val)\n",
    "test_dataset = My_Dataset(test)\n",
    "train_dataset = My_Dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creaing list to hold results\n",
    "results_news = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # print(softmax(logits))\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_news.append({\"Data Split\": \"Train\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # print(softmax(logits))\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_news.append({\"Data Split\": \"Validation\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # print(softmax(logits))\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_news.append({\"Data Split\": \"Test\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting to dataframe\n",
    "results_news = pd.DataFrame(results_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Writing to disk\n",
    "results_news.to_csv(\"results/bert_results_news_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing (Twitter Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the in the splits for each dataset\n",
    "test = pd.read_csv(f\"data/test_{NUM_SAMPLES}_tweets.csv\")\n",
    "val = pd.read_csv(f\"data/val_{NUM_SAMPLES}_tweets.csv\")\n",
    "train = pd.read_csv(f\"data/train_{NUM_SAMPLES}_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepping the data\n",
    "test = prep_data(test)\n",
    "val = prep_data(val)\n",
    "train = prep_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating list to hold results\n",
    "results_twitter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the data as pytorch datasets\n",
    "val_dataset = My_Dataset(val)\n",
    "test_dataset = My_Dataset(test)\n",
    "train_dataset = My_Dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # print(softmax(logits))\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_twitter.append({\"Data Split\": \"Validation\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # print(softmax(logits))\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_twitter.append({\"Data Split\": \"Train\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # print(softmax(logits))\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "results_twitter.append({\"Data Split\": \"Test\", \"F1\": f1.compute()['f1'], \"Accuracy\": accuracy.compute()['accuracy'], \"Precision\": precision.compute()['precision'], \"Recall\": recall.compute()['recall']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_twitter = pd.DataFrame(results_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_twitter.to_csv(\"results/bert_results_twitter_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "results_twitter = pd.read_csv(\"results/bert_results_twitter_data.csv\", index_col=0).drop([\"Precision\", \"Recall\"], axis=1)\n",
    "results_news = pd.read_csv(\"results/bert_results_news_data.csv\", index_col=0).drop([\"Precision\", \"Recall\"], axis=1)\n",
    "results_all = pd.read_csv(\"results/bert_results_all_data.csv\", index_col=0).drop([\"Precision\", \"Recall\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merging the results together\n",
    "results = pd.merge(pd.merge(results_all,results_twitter, on=\"Data Split\"), results_news, on=\"Data Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rounding the datra\n",
    "results = results.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the index\n",
    "results.set_index(\"Data Split\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating multi-indexed columns\n",
    "types = [\"All Data\" for i in range(0,2)] + [\"Sentiment140 Split\" for i in range(0,2)] + [\"NewsMTSC Split\" for i in range(0,2)]\n",
    "columns = [\"Accuracy\", \"F1\"] + [\"Accuracy\", \"F1\"]  + [\"Accuracy\", \"F1\"]\n",
    "multi_index_columns = list(zip(types, columns))\n",
    "results.columns = pd.MultiIndex.from_tuples(multi_index_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = results.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reordering rows\n",
    "results = results.reindex([\"Train\", \"Validation\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">All Data</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Sentiment140 Split</th>\n",
       "      <th colspan=\"2\" halign=\"left\">NewsMTSC Split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           All Data       Sentiment140 Split       NewsMTSC Split      \n",
       "           Accuracy    F1           Accuracy    F1       Accuracy    F1\n",
       "Data Split                                                             \n",
       "Train          0.89   0.9               0.87  0.87           0.92  0.94\n",
       "Validation     0.82  0.82               0.81   0.8           0.82  0.84\n",
       "Test            0.8  0.82                0.8   0.8           0.81  0.84"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/google-chrome\n"
     ]
    }
   ],
   "source": [
    "# Setting styles and writing to disk\n",
    "results = results.style.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "results.set_properties(**{'text-align': 'center'})\n",
    "dfi.export(results, 'results/bert_results_table.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
